\documentclass[a4paper,11pt]{article}

\usepackage{graphicx, url, verbatim}

%--Title---------
\title{Software Evolution - Assignment 4 : Testing and repository mining}
\author{Stef van Schuylenburg (0744314) \& Jules Wulms (0747580)}
\date{\today}
%----------------

\begin{document}

	\maketitle

	\section{Introduction}
		In this assignment we look into social coding and continuous integration. We try to find out how continuous integration, which comes down to building and testing, is done in various GitHub \cite{github} projects and whether we can derive a relation between the way in which changes are introduced to a GitHub repository and the chance of the corresponding build to fail. \\

		In order to get all the information we want, we use a couple of tools. GHTorrent \cite{ghtorrent} provides a MySQL database, which we can use to query the metadata of GitHub repositories. We use several MySQL queries to get data for all large and active independent projects. Information about the continuous integration of these projects, we get using Travis CI \cite{travis}. Travis CI is a distributed continuous integration service for projects hosted at GitHub. We can get information about the builds using the API provided by the official Travis CI web interface. When we have extracted all the data, we use R \cite{rstatistic} to do a statistical analysis on the data. \\

		The report has the following structure: In Section 2 we elaborate on the systems we are analyzing, namely GitHub and Travis CI, to provide some insight in what we are investigating. Section 3 is about the methodology of our research and an in-depth look into the tools and programs we use to get our results. Section 4 describes our results and what facts we can derive from them. In Section 5 we discuss threads that can invalidate our research. The report ends with conclusions we can draw from looking into repositories and continuous integration, and their relation to evolution.
	
	\section{Background}
		As stated in the previous section, we look at repositories on Github and we want to get information about the build
		made by Travis CI.
		
		\subsection{GitHub}
			Github\cite{github} is a platform that is build around the version control system git.
			It allows users to host their git repositories and it supplies a web interface for browsing through repositories and
			to manage repositories.
			Github calls their platform \emph{social coding}, by which they mean that they want to provide services such that
			developers can easily share code, discuss changes and collaborate.
			Github is very popular for open source projects, but it is also used for commercial projects, where companies can pay
			to create private projects.
			At the moment of writing Github is the largest code host and hosts over 11.8 million repositories.
			
			One particular feature of Github we look at in this assignment are the pull requests: 
			For git normal changes are saved as commits (similar to other version control systems),
			but Github allows users to group a number of commits as a pull requests, which must be first accepted before the
			changes are applied to the main version of the repository.
			Those pull requests are introduced such that the changes can be checked on quality before commiting them to the main
			version.
		
		\subsection{Travis CI}
		  Travis CI\cite{travis} is a Continuous Integration system for github.
		  Travis is used to automatically build and test Github project each time a commit has been made or a pull request
		  is accepted.
		  Everytime a build is done, Travis shows the state and keeps a history of the builds which we use in this assignment
		  to do calculations with.
		  Travis is used primarly for open source projects as the main version of Travis does not support private
		  repositories.
		  There is a version of Travis, that does support private repositories, but this system is not widely used yet.
		  
		  
		
	\section{Methodology}
		Now that we know what we are looking into, we can proceed by elaborating on what steps we have to take to extract and analyse data from the systems we are considering. The process consists of three steps: extracting data about GitHub projects throught GHTorrent's MySQL database, extracting continuous integration data via Travis CI, and doing statistical tests using R. Each of the following subsections will elaborate on a different part of this process.
		
		\subsection{Identifying GitHub projects}
			We want to extract data about GitHub projects using the GHTorrent MySQL database. The database is has a web interface available at \url{http://ghtorrent.org/dblite/}. For the later analysis, it is important to make the amount of projects managable, and in order to get useful data, we query only specific projects, which are "large and active independent projects". The projects we want all have to meet following requirements, such that they:

			\begin{enumerate}
				\item are not forks of other repositories,
				\item have not been deleted,
				\item have at least 10 different contributors,
				\item have at least 10 changes during the previous month, where changes are commits or pull requests,
				\item are at least one year old,
				\item have both commits and pull requests,
				\item have been developed in Java, Python or Ruby.
			\end{enumerate}
	
			In one big query, we are able to fullfil all the requirements, except the forth requirement. 
			The main query can be	found in Appendix \ref{app:mainquery}, it has a statement/subquery in the where clause for each of the other requirements. 
			The forth requirement needs further processing of the commit and pull request tables in the database, which are really big tables. 
			Processing these tables in the main query is very inefficient and results in very long running times. 
			Since the web interface of GHTorrent's MySQL database was very unstable, when we used it, we decided against a query with too long running times. 
			The alternative was smaller helper queries for the more complex data we wanted to extract, which involved counting rows. 
			The queries are shown in Appendix \ref{app:mainquery} and use the main query as a source of pruning the id values of the projects that have to be considered.\\

			When all the data is extracted from the GHTorrents database, we put the extracted .csv-files in an Excel-file, and use the VLOOKUP-function in Microsoft Excel to combine the results of the same projects, using their id values. When all the results are combined, we add the number of commits and pull requests in a new column to get the changes that we need for requirement 4. Then we can easily sort the table on this changes column, and delete all entries that have a value lower than 10. At this point, we have identified the large and active independent projects, and have already gathered a big chunk of data about them.

		\subsection{Travis CI extraction}
			In the next step we want information about the builds of the Github project.
			For this we use a Ruby library from the Travis CI Client\cite{travis.rb}.
			This library gives us access to information about the builds in Travis CI of a Github project.
			To use this library we need the slugs of the github repositories.
			The slug is a part of the URL to the github, in which the user and the repository name are used (for example for the
			Travis CI Client this would be \texttt{travis-ci/travis.rb}).
			To get those slugs we read the comma seperated values file, containing all the Github projects that met the
			requirements and we extract the slugs from the \texttt{url} collumn in that file (we extract the slug using regular
			expressions).
			
			Now we got all the slugs, we can use the Travis CI Client library to try to find the Github project in Travis.
			If we find the project in Travis then we go through the build history and count the number of passed and failed
			builds for both the pull requests and the commits (we count all the values separated, so we end with four different
			values).
			Then we look for the Travis CI identifier correspending to this Github project.
			
			And in the end we write for all the Github repositories which we can find in Travis CI the Travic CI identifier and
			the build values to a comma seperated values file, such that we can use it in R for the statistical analysis.
			
			% TODO: do we add the Ruby script as appendix or do we add it on Peach?
			The Ruby script we used to extract the information from Travic CI can be found in Appendix \ref{app:ci-extraction}.
	
		\subsection{Statistical analysis}
			Now that we found information about the build history, we want to do a statistical analysis of this data to find
			the dependencies between the number of passed and failed builds of commits and pull requests.
			
			In this step we want to do a $\chi^2$ test and we want to calculate the odds-ratio for each of the repositories of
			which we found the build history.
			To achieve this we use R\cite{r}.
			R is a software environment which we use for statistical computing.
			We use two functions from R for our analysis: 
			\texttt{chisq.test} and \texttt{oddsratio} (the last one comes from the epitools package).
			
			For each of the repositories we create a contingency table filled with the build information,
			which we give as input to both of the functions.
			For our contingency table we look at two properties of the builds: passed or failed and corresponding to a commit or
			a pull request.
			Then we create our table as follow:
			
			\begin{center}
			\begin{tabular}{ccc}
			 & passed & failed\\
			commits & $n_{commit, passed}$ & $n_{commit, failed}$\\
			pull requests & $n_{pull request, passed}$ & $n_{pull request, failed}$
			\end{tabular}
			\end{center}
			
			The values in table are calculated by the Ruby script as exlained in the previous section.
			
			Now we use Pearson's $\chi^2$ test using \texttt{chisq.test} from which we take the p-value and the residuals.
			The p-value corresponds to the significance of this test and the residuals indicate for each combination of events a measurement of how far they are off the expected value.
			We calculate the odds ratio using \texttt{oddsratio} from which we take again the p-value and a measure which
			indicate the strength of dependence.
		  Those values are then saved to a file, which is used for our discussion of the results.
		  
		  To be able to do reliable tests, we do not consider the repositories where one of the $n$-values is lower than five.
		  
		  The R-script we use to calculate those values can be found in Appendix \ref{app:r}.
			
	
	\section{Results and discussion}
		In this section we will present our results and discuss their meaning. We start by looking into the raw data that we collected and after that look further into the results of our statistical analysis.
		
		\subsection{Data Analysis}
			First we will look into the GitHub data that we extracted from the GHTorrent database. We have identified 1839 projects in the main query, of which 223 had enough commits and pull request in the last month, to be considered in the rest of the analysis. An important fact is that we use \texttt{NOW()} in the queries, which makes them dependent on the moment they were executed. We ran all the queries the 30th of March 2014. Of the 223 projects that we are going to check Travis CI data for, 70 are developed in Java, 83 in Python and 73 in Ruby. Java projects seem too use the least pull requests in the last month, with a maximum of 26 and the most projects that use no pull request out of the 3 languages. Python and Ruby both have higher amounts of pull requests, with maxima of 97 and 236 respectively, and less projects that use no pull requests. Some Python and Ruby projects even have more pull requests than commits. \\

			The Shared Repository Model of GitHub is more popular for Java projects, while Python and Ruby projects have more users sending pull requests. A reason for this could be that Python and Ruby are scripting languages, of which projects are probably more sought after by webdevelopers. Web developers usually promote usages of existing functionality and building your own project out of existing building blocks. In this process, bugs can be found or useful functionality can be added to the fork, which can lead to more pull requests. Overall, we see that commits (direct code modification) is more popular than pull requests (indirect code modification), with only a small amount of projects having more pull requests than commits. \\

			When we look into the data we got from Travis CI, we see that 17 out of our 223 projects did not use Travis, of which the most where Java projects (11 in total).  About 100 projects were using Travis CI, since we could find them and extract a Travis ID, but they have no builds on Travis, only failed ones, or errored builds, which we filtered out. These projects could have configured Travis not to build, or have other settings that are set in the wrong way, which prevents us from getting data. When we look into the distribution of these projects with little to none builds, with respect to programming language, we see for Java, Python and Ruby respectively a 40-40-20 split. It is strange that out of all projects, the Java projects use Travis the least, since it automatically runs things like unit tests for you, and Java has great support for this.\\

			Outside of all the projects that were not using Travis, or didn't use it in the right way, there were projects that just had too little data to fill the contingency table. In most cases, the failed pull request had too little data. This can be explained by the fact that there are less pull requests overall, but we think there is a clearer reason: when a developer has no commit rights, and just suggests a change via a pull request, he/she will try harder to make sure it is a valid change and will not break anything. When the developer has commit rights, he can try out new things more easily, since he also has the power to reverse it again. There is no strong difference between the programming languages, however we can for example see that some Python and Ruby projects had more pull requests and less commits than average, sometimes have too little failed commits, while the other values are high enough. In the end, we can use 84 projects for the statistical analysis. \\
			
		\subsection{Discussion of Statistical Analysis}
			The next step is to look into the statistical measurements and see if we can derive a relation between the way changes are made to a repository and the chance that the corresponding build fails. Since we use the $\chi^2$ test, we need a null hypothesis, which will be that the way changes are made to the repository is independent from the chance the build fails. The odds ratio tells us whether it is more likely that a build that passes the tests comes from a commit or a pull request, if it is above 1, passed builds are more likely to come from a commit, if it is below 1, passed builds are more likely to come from a pull request. \\

			We use the p-value of the $\chi^2$ test to see if the test was statistically significant, and if not, we can reject the null hypothesis. This means that we are not sure whether the way of changing a repository is influenced by the correctness of the build. We cannot derive anything else from this! Furthermore, we can look into the odds ratio's p-value, too see if it is statistically significant. If so, we can use it to state how likely it is that the way of chaning influences correctness of build. \\
			
			Now that we know what to look for we can start looking into the data. When we take $0.05$ as a threshold for the p-values, we see that for 45 projects, we can reject the null hypothesis, while for 39 projects we do not reject the null hypothesis with 95\% confidence. For all but one project where we cannot reject the null hypothesis, the p-value for the odds ratio also indicates that it is statistically significant. In most cases, the odds ratio is bigger than 1, which means that passed builds are more likely to come from a commit. However, the values are usually pretty close to 1, with only 8 projects having a value bigger than 1.4, and 10 below 1.\\

			If we look at the different languages, in Java we see 7 out of 10 projects where we cannot reject the null hypothesis, for Python 15 out of 34 and for Ruby 17 out of 40. So we see that for Java there are clearly more projects where we cannot reject the null hypothesis (70\%), while Python and Ruby are fairly close (42-44\%). For Java and Ruby the same projects have a statistically significant odds ratio, while Python has 1 less project with a statistically significant odds ratio. Java has 2 values below 1, and a 2.1 spike in the odds ratio values, while Python has 6 below 1 and no significant spike, and Ruby has 3 values below 1. Here we see that for Python, in more cases we see that passed builds are more likely to come from pull requests (40\%), while Java (29\%) and Ruby (18\%) have lower amount of cases where this indication can be found. \\

			The age of the project does not seem to correlate with the relation we are looking for. When looking at the 4 oldest repositories in our dataset, 2 of them have p-values lower than 0.001, and 2 bigger than 0.8. In the younger projects, we see the same kind of mixture of significant and insignificant results, and residuals and odss ratios do not seem to increase or decrease over time. \\

			
			
	\section{Threads to validity}
		Now 
	
	\section{Conclusions}
		W.

	\appendix
	\section{Main query}
	\label{app:mainquery}
	This is the main query to get a list of GitHub repositories that meet all requirements for large and active independent repositories, except for one: at least 10 changes (commits/pull requests) in the past month. The SELECT-statement is big to extract as much data as we can for the analysis.
	
	{\scriptsize
	\begin{verbatim}
SELECT id, url, name, language, 
       TIMESTAMPDIFF(day, DATE_FORMAT(projects.created_at,"\%Y-\%m-\%d"), "2014-03-06") as age_days 
FROM projects 
WHERE -- not deleted
      projects.deleted = 0 AND 
      -- not a fork of another project
      projects.forked_from IS NULL AND 
      -- at least 1 year old
      projects.created_at <= DATE_SUB(NOW(),INTERVAL 1 YEAR) AND 
      -- written in Java/Ruby/Python
     (projects.language = "Java" OR projects.language = "Ruby" OR projects.language = "Python") AND 
      -- at least 10 different users
      EXISTS (SELECT COUNT(DISTINCT user_id) as usercount 
              FROM project_members 
              WHERE project_members.repo_id = projects.id  
              HAVING usercount >= 10) AND 
      -- at least 1 commit and 1 pull request
      EXISTS (SELECT pull_requests.id, commits.id 
              FROM pull_requests, commits 
              WHERE pull_requests.base_repo_id = projects.id AND 
                    commits.project_id = projects.id);
	\end{verbatim}
	}
	
	\section{Helper queries}
	\label{app:helpquery}
	The helper queries fullfill the requirement that was not met in the main query, and some queries get extra data for the analysis. All the queries use the main query as a way to find a smaller set of project id's to check. However, the main query as subquery was too complex for the commits helper query, since the commits table has over 90 million entries. There we used a list of comma separated id values, custom made by copying the id column of the main query, and replacing newlines by commas, using Notepad++.
	
	{\scriptsize
	\begin{verbatim}
-- amount of contributors per repo (using unfiltered id's)
SELECT repo_id, COUNT(user_id) as contributors 
FROM project_members 
WHERE repo_id IN (SELECT id  
                  FROM projects 
                  WHERE projects.deleted = 0 AND 
                        projects.forked_from IS NULL AND 
                        projects.created_at <= DATE_SUB(NOW(),INTERVAL 1 YEAR) AND 
                       (projects.language = "Java" OR projects.language = "Ruby" OR 
                        projects.language = "Python") AND 
                        EXISTS (SELECT COUNT(DISTINCT user_id) as usercount 
                                FROM project_members 
                                WHERE project_members.repo_id = projects.id  
                                HAVING usercount >= 10) AND 
                        EXISTS (SELECT pull_requests.id, commits.id 
                                FROM pull_requests, commits 
                                WHERE pull_requests.base_repo_id = projects.id AND 
                                      commits.project_id = projects.id)) 
GROUP BY repo_id 
ORDER BY repo_id ASC;

-- amount of pull requests last month (using unfiltered id's)
SELECT base_repo_id, COUNT(pull_requests.id) as pull_reqs_last_month 
FROM pull_requests, pull_request_history 
WHERE pull_requests.id = pull_request_history.pull_request_id AND 
      pull_request_history.action = "opened" and 
      pull_request_history.created_at >= DATE_SUB(NOW(), INTERVAL 1 MONTH) AND 
      pull_requests.base_repo_id IN (SELECT id 
                  FROM projects 
                  WHERE projects.deleted = 0 AND 
                        projects.forked_from IS NULL AND 
                        projects.created_at <= DATE_SUB(NOW(),INTERVAL 1 YEAR) AND 
                       (projects.language = "Java" OR projects.language = "Ruby" OR 
                        projects.language = "Python") AND 
                        EXISTS (SELECT COUNT(DISTINCT user_id) as usercount 
                                FROM project_members 
                                WHERE project_members.repo_id = projects.id 
                                HAVING usercount >= 10) AND 
                        EXISTS (SELECT pull_requests.id, commits.id 
                                FROM pull_requests, commits 
                                WHERE pull_requests.base_repo_id = projects.id AND 
                                      commits.project_id = projects.id)) 
GROUP BY base_repo_id 
ORDER BY base_repo_id ASC;

-- amount of commits last month (using unfiltered id's)
SELECT project_id, COUNT(id) as commits_last_month 
FROM commits 
WHERE commits.created_at >= DATE_SUB(NOW(), INTERVAL 1 MONTH) AND 
      project_id IN (--id's--) 
GROUP BY project_id 
ORDER BY project_id ASC;
	\end{verbatim}
	}
	
	\section{Travis CI Extraction script}
	\label{app:ci-extraction}
	This is the Ruby script that executes the Travis CI Extraction and saves it to a comma seperated values file.
	It contains one TravisReader class which is used to calculate the data about the Travis build history.
	
	{\scriptsize 
		\verbatiminput{../ruby/main.rb}
		\verbatiminput{../ruby/travis_reader.rb}
	}
	
	\section{Statistical Analysis script}
	\label{app:r}
	This is the R script that does the statistical computations and saves it to a file.
	{\scriptsize
	  \verbatiminput{../r/analysis.r} 
	}
	
	

	\begin{thebibliography}{9}
		\bibitem{github}
			GitHub, Web-based hosting service for software development projects using Git revision control, https://github.com/

		\bibitem{ghtorrent}
			Gousios, Georgios, The GHTorrent dataset and tool suite, 
			Proceedings of the 10th Working Conference on Mining Software Repositories, MSR '13,
			San Francisco, CA, USA, IEEE Press, 2013

		\bibitem{travis}
			Travis CI, a hosted, distributed continuous integration service for GitHub projects, https://travis-ci.org/

		\bibitem{rstatistic}
			R, language and environment for statistical computing and graphics, http://http://www.r-project.org/
			
		\bibitem{travis.rb}
		  The Travis CI client, a tool (both a command line client and a Ruby library) to interface with a Travis CI service,
		  https://github.com/travis-ci/travis.rb
		  
		\bibitem{r}
		  R, software environment for statistical computing, http://www.r-project.org/
		
	\end{thebibliography}

\end{document}